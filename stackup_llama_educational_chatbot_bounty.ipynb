{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkekw9MKl18PQxp3EZpw31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyprian-kiplangat/AI-powered-educational-chatbot/blob/main/stackup_llama_educational_chatbot_bounty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Educational AI Tutor Bot using Llama 2 model and sentiment analysis."
      ],
      "metadata": {
        "id": "1FK19WpVQHs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate protobuf sentencepiece torch transformers huggingface_hub gradio"
      ],
      "metadata": {
        "id": "MSTXCmZRQC7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Logging in to Hugging Face"
      ],
      "metadata": {
        "id": "ubM4pC7xImVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "5RasqfAHInrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Hugging Face login token (add your token here)\n",
        "login(token=\"\")"
      ],
      "metadata": {
        "id": "b4AXr7tAIwmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup: Define sentiment analysis model and Llama 2 model"
      ],
      "metadata": {
        "id": "xYhzKIkRKrsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model_id = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model_id, tokenizer=sentiment_model_id)\n",
        "\n",
        "llama_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(llama_model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "llama_pipeline = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer, max_length=1024)"
      ],
      "metadata": {
        "id": "631Jac_HKwG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global conversation history and sentiment prompts\n",
        "conversation_history = []\n",
        "sentiment_prompts = {\n",
        "            \"Positive\": \"The student seems to understand well. Encourage deeper exploration while maintaining their enthusiasm.\",\n",
        "            \"Neutral\": \"\",\n",
        "            \"Negative\": \"The student is struggling. Offer encouragement, simplify explanations, and check for understanding frequently.\"\n",
        "}"
      ],
      "metadata": {
        "id": "z9QZsrPqQwPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment analysis function"
      ],
      "metadata": {
        "id": "Z_q9t-uZNZ43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the provided text using the sentiment analysis pipeline.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: Sentiment label - Positive, Neutral, or Negative.\n",
        "    \"\"\"\n",
        "  sentiment_result = sentiment_pipeline(text)[0]['label']\n",
        "  if sentiment_result == 'LABEL_1':\n",
        "    return 'Neutral'\n",
        "  elif sentiment_result == 'LABEL_2':\n",
        "    return 'Positive'\n",
        "  else:\n",
        "    return 'Negative'\n"
      ],
      "metadata": {
        "id": "wPI6WXIROzEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt creation function"
      ],
      "metadata": {
        "id": "Ts--U3t_S14z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question, sentiment):\n",
        "    \"\"\"\n",
        "    Generates a prompt for the Llama model based on the user's question and sentiment.\n",
        "\n",
        "    Args:\n",
        "        question (str): User's input question.\n",
        "        sentiment (str): Sentiment label (Positive, Neutral, or Negative).\n",
        "\n",
        "    Returns:\n",
        "        str: Generated prompt for the model.\n",
        "    \"\"\"\n",
        "  sentiment_guidance = sentiment_prompts[sentiment]\n",
        "  recent_context = ' '.join(conversation_history[-5:]) # Last 5 messages for context\n",
        "  prompt = f\"\"\"\n",
        "  You are an educational AI tutor capable of assisting with various subjects and topics.\n",
        "  {sentiment_prompts[sentiment]}\n",
        "  Recent context: {recent_context}\n",
        "  Student: {question}\n",
        "  Assistant:\n",
        "  \"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "H7XwNbQyS5d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate response function\n"
      ],
      "metadata": {
        "id": "5PA7NGCYRg68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question):\n",
        "    \"\"\"\n",
        "    Generates the assistant's response based on the user's question using sentiment analysis\n",
        "    and the Llama model.\n",
        "\n",
        "    Args:\n",
        "        question (str): User's input question.\n",
        "\n",
        "    Returns:\n",
        "        str: Assistant's response.\n",
        "    \"\"\"\n",
        "  sentiment = analyze_sentiment(question) # Determine the sentiment of the user's input\n",
        "  prompt = create_prompt(question, sentiment)  # Create the prompt based on sentiment\n",
        "\n",
        "  response = llama_pipeline(prompt, do_sample=True, truncation=True, num_return_sequences=1)[0]['generated_text']\n",
        "  assistant_response = response.split(\"Assistant:\")[-1].strip() # Extract the assistant's reply\n",
        "\n",
        "  # Add the conversation to history for future context\n",
        "  conversation_history.append(f\"Student: {question}\")\n",
        "  conversation_history.append(f\"Assistant: {assistant_response}\")\n",
        "\n",
        "  return assistant_response"
      ],
      "metadata": {
        "id": "hvKJSlJrRf_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio interface function\n"
      ],
      "metadata": {
        "id": "3B0cZ1Ahb1up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_interface(question):\n",
        "    \"\"\"\n",
        "    Gradio interface function to generate a chatbot response for the given question.\n",
        "\n",
        "    Args:\n",
        "        question (str): User's input question.\n",
        "\n",
        "    Returns:\n",
        "        str: Assistant's response.\n",
        "    \"\"\"\n",
        "\n",
        "  response = generate_response(question)\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "EG9fsG2ub4eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gradio interface with title, description, inputs, and outputs\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Education Tutor Bot \",\n",
        "    description=\"Ask a question and the chatbot will respond depending on your learning style and mood\",\n",
        ")"
      ],
      "metadata": {
        "id": "JE86xFlxelRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "pb0G1YyAeneh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}